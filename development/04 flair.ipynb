{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flair\n",
    "flair.set_seed(2)\n",
    "\n",
    "from flair.data import Corpus, Sentence\n",
    "from flair.datasets import TREC_6, CSVClassificationCorpus\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5.0e-5\n",
    "mini_batch_size = 4\n",
    "max_epochs = 10\n",
    "model_name = \"deepvk/USER-bge-m3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 19:24:45,923 Reading data from data\\flair_data\n",
      "2024-11-16 19:24:45,923 Train: data\\flair_data\\train.csv\n",
      "2024-11-16 19:24:45,924 Dev: data\\flair_data\\dev.csv\n",
      "2024-11-16 19:24:45,925 Test: data\\flair_data\\test.csv\n",
      "2024-11-16 19:24:45,938 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "4068it [00:01, 3985.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 19:24:46,967 Dictionary created for label 'label' with 9 values: FOOD_GOODS (seen 922 times), NON_FOOD_GOODS (seen 895 times), SERVICE (seen 884 times), LEASING (seen 380 times), LOAN (seen 380 times), REALE_STATE (seen 256 times), BANK_SERVICE (seen 215 times), NOT_CLASSIFIED (seen 117 times), TAX (seen 19 times)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "        \n",
    "column_name_map = {0: \"text\", 1: \"label\"}\n",
    "corpus: Corpus = CSVClassificationCorpus(\"data/flair_data\",\n",
    "                                            column_name_map,\n",
    "                                            skip_header=False,\n",
    "                                            delimiter='\\t',    # tab-separated files\n",
    "                                            label_type='label')\n",
    "\n",
    "\n",
    "\n",
    "label_dict = corpus.make_label_dictionary(label_type='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 19:24:50,923 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-16 19:24:50,925 Model: \"TextClassifier(\n",
      "  (embeddings): TransformerDocumentEmbeddings(\n",
      "    (model): XLMRobertaModel(\n",
      "      (embeddings): XLMRobertaEmbeddings(\n",
      "        (word_embeddings): Embedding(46167, 1024, padding_idx=1)\n",
      "        (position_embeddings): Embedding(8194, 1024, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 1024)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): XLMRobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-23): 24 x XLMRobertaLayer(\n",
      "            (attention): XLMRobertaAttention(\n",
      "              (self): XLMRobertaSdpaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): XLMRobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): XLMRobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): XLMRobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): XLMRobertaPooler(\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Linear(in_features=1024, out_features=9, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (locked_dropout): LockedDropout(p=0.0)\n",
      "  (word_dropout): WordDropout(p=0.0)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2024-11-16 19:24:50,926 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-16 19:24:50,927 Corpus: 4068 train + 500 dev + 500 test sentences\n",
      "2024-11-16 19:24:50,927 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-16 19:24:50,928 Train:  4068 sentences\n",
      "2024-11-16 19:24:50,929         (train_with_dev=False, train_with_test=False)\n",
      "2024-11-16 19:24:50,929 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-16 19:24:50,930 Training Params:\n",
      "2024-11-16 19:24:50,931  - learning_rate: \"5e-05\" \n",
      "2024-11-16 19:24:50,931  - mini_batch_size: \"4\"\n",
      "2024-11-16 19:24:50,932  - max_epochs: \"10\"\n",
      "2024-11-16 19:24:50,933  - shuffle: \"True\"\n",
      "2024-11-16 19:24:50,933 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-16 19:24:50,934 Plugins:\n",
      "2024-11-16 19:24:50,934  - LinearScheduler | warmup_fraction: '0.1'\n",
      "2024-11-16 19:24:50,935 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-16 19:24:50,936 Final evaluation on model after last epoch (final-model.pt)\n",
      "2024-11-16 19:24:50,936  - metric: \"('micro avg', 'f1-score')\"\n",
      "2024-11-16 19:24:50,937 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-16 19:24:50,937 Computation:\n",
      "2024-11-16 19:24:50,938  - compute on device: cuda:0\n",
      "2024-11-16 19:24:50,938  - embedding storage: none\n",
      "2024-11-16 19:24:50,939 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-16 19:24:50,940 Model training base path: \"models\\gpt_data_v2\"\n",
      "2024-11-16 19:24:50,941 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-16 19:24:50,941 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-16 19:25:29,334 epoch 1 - iter 101/1017 - loss 1.88612350 - time (sec): 38.39 - samples/sec: 10.52 - lr: 0.000005 - momentum: 0.000000\n",
      "2024-11-16 19:25:59,370 epoch 1 - iter 202/1017 - loss 1.19980252 - time (sec): 68.43 - samples/sec: 11.81 - lr: 0.000010 - momentum: 0.000000\n",
      "2024-11-16 19:26:27,795 epoch 1 - iter 303/1017 - loss 0.86400049 - time (sec): 96.85 - samples/sec: 12.51 - lr: 0.000015 - momentum: 0.000000\n",
      "2024-11-16 19:27:06,284 epoch 1 - iter 404/1017 - loss 0.67100750 - time (sec): 135.34 - samples/sec: 11.94 - lr: 0.000020 - momentum: 0.000000\n",
      "2024-11-16 19:27:34,637 epoch 1 - iter 505/1017 - loss 0.57691962 - time (sec): 163.69 - samples/sec: 12.34 - lr: 0.000025 - momentum: 0.000000\n",
      "2024-11-16 19:28:21,759 epoch 1 - iter 606/1017 - loss 0.50922213 - time (sec): 210.82 - samples/sec: 11.50 - lr: 0.000030 - momentum: 0.000000\n",
      "2024-11-16 19:29:13,256 epoch 1 - iter 707/1017 - loss 0.46392307 - time (sec): 262.31 - samples/sec: 10.78 - lr: 0.000035 - momentum: 0.000000\n",
      "2024-11-16 19:29:46,304 epoch 1 - iter 808/1017 - loss 0.42170545 - time (sec): 295.36 - samples/sec: 10.94 - lr: 0.000040 - momentum: 0.000000\n",
      "2024-11-16 19:31:11,386 epoch 1 - iter 909/1017 - loss 0.38624699 - time (sec): 380.44 - samples/sec: 9.56 - lr: 0.000045 - momentum: 0.000000\n",
      "2024-11-16 19:31:42,702 epoch 1 - iter 1010/1017 - loss 0.36681397 - time (sec): 411.76 - samples/sec: 9.81 - lr: 0.000050 - momentum: 0.000000\n",
      "2024-11-16 19:31:44,942 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-16 19:31:44,943 EPOCH 1 done: loss 0.3643 - lr: 0.000050\n",
      "2024-11-16 19:31:44,944 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:09<00:00,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 19:31:56,601 DEV : loss 0.4902525246143341 - f1-score (micro avg)  0.922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 32/32 [00:11<00:00,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 19:32:08,390 TEST : loss 0.4902525246143341 - f1-score (micro avg)  0.922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 19:32:08,713 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-16 19:32:41,688 epoch 2 - iter 101/1017 - loss 0.11498597 - time (sec): 32.97 - samples/sec: 12.25 - lr: 0.000049 - momentum: 0.000000\n",
      "2024-11-16 19:33:22,549 epoch 2 - iter 202/1017 - loss 0.15942758 - time (sec): 73.83 - samples/sec: 10.94 - lr: 0.000049 - momentum: 0.000000\n",
      "2024-11-16 19:34:06,986 epoch 2 - iter 303/1017 - loss 0.21001581 - time (sec): 118.27 - samples/sec: 10.25 - lr: 0.000048 - momentum: 0.000000\n",
      "2024-11-16 19:34:48,703 epoch 2 - iter 404/1017 - loss 0.23103512 - time (sec): 159.99 - samples/sec: 10.10 - lr: 0.000048 - momentum: 0.000000\n",
      "2024-11-16 19:35:32,404 epoch 2 - iter 505/1017 - loss 0.21175898 - time (sec): 203.69 - samples/sec: 9.92 - lr: 0.000047 - momentum: 0.000000\n",
      "2024-11-16 19:36:13,295 epoch 2 - iter 606/1017 - loss 0.23226221 - time (sec): 244.58 - samples/sec: 9.91 - lr: 0.000047 - momentum: 0.000000\n",
      "2024-11-16 19:36:56,735 epoch 2 - iter 707/1017 - loss 0.22397422 - time (sec): 288.02 - samples/sec: 9.82 - lr: 0.000046 - momentum: 0.000000\n",
      "2024-11-16 19:38:02,888 epoch 2 - iter 808/1017 - loss 0.47352034 - time (sec): 354.17 - samples/sec: 9.13 - lr: 0.000046 - momentum: 0.000000\n",
      "2024-11-16 19:38:53,461 epoch 2 - iter 909/1017 - loss 0.64183016 - time (sec): 404.74 - samples/sec: 8.98 - lr: 0.000045 - momentum: 0.000000\n",
      "2024-11-16 19:39:58,015 epoch 2 - iter 1010/1017 - loss 0.72940579 - time (sec): 469.30 - samples/sec: 8.61 - lr: 0.000044 - momentum: 0.000000\n",
      "2024-11-16 19:40:01,720 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-16 19:40:01,721 EPOCH 2 done: loss 0.7244 - lr: 0.000044\n",
      "2024-11-16 19:40:01,722 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:14<00:00,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 19:40:18,248 DEV : loss 0.07244223356246948 - f1-score (micro avg)  0.974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 32/32 [00:13<00:00,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 19:40:32,290 TEST : loss 0.07244223356246948 - f1-score (micro avg)  0.974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 19:40:32,585 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-16 19:41:36,624 epoch 3 - iter 101/1017 - loss 0.12431403 - time (sec): 64.04 - samples/sec: 6.31 - lr: 0.000044 - momentum: 0.000000\n",
      "2024-11-16 19:42:32,641 epoch 3 - iter 202/1017 - loss 0.16756819 - time (sec): 120.05 - samples/sec: 6.73 - lr: 0.000043 - momentum: 0.000000\n",
      "2024-11-16 19:43:40,468 epoch 3 - iter 303/1017 - loss 0.14886173 - time (sec): 187.88 - samples/sec: 6.45 - lr: 0.000043 - momentum: 0.000000\n",
      "2024-11-16 19:45:35,559 epoch 3 - iter 404/1017 - loss 0.18702503 - time (sec): 302.97 - samples/sec: 5.33 - lr: 0.000042 - momentum: 0.000000\n",
      "2024-11-16 19:46:23,517 epoch 3 - iter 505/1017 - loss 0.26844292 - time (sec): 350.93 - samples/sec: 5.76 - lr: 0.000042 - momentum: 0.000000\n",
      "2024-11-16 19:47:14,228 epoch 3 - iter 606/1017 - loss 0.55129664 - time (sec): 401.64 - samples/sec: 6.04 - lr: 0.000041 - momentum: 0.000000\n",
      "2024-11-16 19:48:04,331 epoch 3 - iter 707/1017 - loss 0.75249638 - time (sec): 451.74 - samples/sec: 6.26 - lr: 0.000041 - momentum: 0.000000\n",
      "2024-11-16 19:48:51,835 epoch 3 - iter 808/1017 - loss 0.90821971 - time (sec): 499.25 - samples/sec: 6.47 - lr: 0.000040 - momentum: 0.000000\n",
      "2024-11-16 19:49:40,864 epoch 3 - iter 909/1017 - loss 1.02343665 - time (sec): 548.28 - samples/sec: 6.63 - lr: 0.000039 - momentum: 0.000000\n",
      "2024-11-16 19:50:45,516 epoch 3 - iter 1010/1017 - loss 1.11361172 - time (sec): 612.93 - samples/sec: 6.59 - lr: 0.000039 - momentum: 0.000000\n",
      "2024-11-16 19:50:48,239 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-16 19:50:48,239 EPOCH 3 done: loss 1.1190 - lr: 0.000039\n",
      "2024-11-16 19:50:48,241 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:09<00:00,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 19:51:00,286 DEV : loss 2.38578462600708 - f1-score (micro avg)  0.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 32/32 [00:11<00:00,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 19:51:11,762 TEST : loss 2.38578462600708 - f1-score (micro avg)  0.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 19:51:11,962 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-16 19:52:05,316 epoch 4 - iter 101/1017 - loss 1.53851904 - time (sec): 53.35 - samples/sec: 7.57 - lr: 0.000038 - momentum: 0.000000\n",
      "2024-11-16 19:53:02,600 epoch 4 - iter 202/1017 - loss 0.90714725 - time (sec): 110.63 - samples/sec: 7.30 - lr: 0.000038 - momentum: 0.000000\n",
      "2024-11-16 19:54:04,036 epoch 4 - iter 303/1017 - loss 0.69619727 - time (sec): 172.07 - samples/sec: 7.04 - lr: 0.000037 - momentum: 0.000000\n",
      "2024-11-16 19:54:58,025 epoch 4 - iter 404/1017 - loss 0.56985694 - time (sec): 226.06 - samples/sec: 7.15 - lr: 0.000037 - momentum: 0.000000\n",
      "2024-11-16 19:56:10,547 epoch 4 - iter 505/1017 - loss 0.48337150 - time (sec): 298.58 - samples/sec: 6.77 - lr: 0.000036 - momentum: 0.000000\n",
      "2024-11-16 19:57:06,679 epoch 4 - iter 606/1017 - loss 0.41400839 - time (sec): 354.71 - samples/sec: 6.83 - lr: 0.000036 - momentum: 0.000000\n",
      "2024-11-16 19:58:01,861 epoch 4 - iter 707/1017 - loss 0.36490281 - time (sec): 409.90 - samples/sec: 6.90 - lr: 0.000035 - momentum: 0.000000\n",
      "2024-11-16 19:58:55,422 epoch 4 - iter 808/1017 - loss 0.33548676 - time (sec): 463.46 - samples/sec: 6.97 - lr: 0.000034 - momentum: 0.000000\n",
      "2024-11-16 19:59:49,727 epoch 4 - iter 909/1017 - loss 0.31001504 - time (sec): 517.76 - samples/sec: 7.02 - lr: 0.000034 - momentum: 0.000000\n",
      "2024-11-16 20:01:11,826 epoch 4 - iter 1010/1017 - loss 0.28963947 - time (sec): 599.86 - samples/sec: 6.73 - lr: 0.000033 - momentum: 0.000000\n",
      "2024-11-16 20:01:13,753 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-16 20:01:13,754 EPOCH 4 done: loss 0.2897 - lr: 0.000033\n",
      "2024-11-16 20:01:13,756 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:09<00:00,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 20:01:24,490 DEV : loss 0.016033632680773735 - f1-score (micro avg)  0.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 32/32 [00:09<00:00,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 20:01:33,990 TEST : loss 0.016033632680773735 - f1-score (micro avg)  0.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 20:01:34,162 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-16 20:02:23,309 epoch 5 - iter 101/1017 - loss 0.04357185 - time (sec): 49.14 - samples/sec: 8.22 - lr: 0.000033 - momentum: 0.000000\n",
      "2024-11-16 20:03:14,731 epoch 5 - iter 202/1017 - loss 0.03028550 - time (sec): 100.57 - samples/sec: 8.03 - lr: 0.000032 - momentum: 0.000000\n",
      "2024-11-16 20:04:08,463 epoch 5 - iter 303/1017 - loss 0.03590309 - time (sec): 154.30 - samples/sec: 7.85 - lr: 0.000032 - momentum: 0.000000\n",
      "2024-11-16 20:04:55,976 epoch 5 - iter 404/1017 - loss 0.04397541 - time (sec): 201.81 - samples/sec: 8.01 - lr: 0.000031 - momentum: 0.000000\n",
      "2024-11-16 20:05:47,254 epoch 5 - iter 505/1017 - loss 0.04124751 - time (sec): 253.09 - samples/sec: 7.98 - lr: 0.000031 - momentum: 0.000000\n",
      "2024-11-16 20:06:35,673 epoch 5 - iter 606/1017 - loss 0.04685602 - time (sec): 301.51 - samples/sec: 8.04 - lr: 0.000030 - momentum: 0.000000\n",
      "2024-11-16 20:07:28,405 epoch 5 - iter 707/1017 - loss 0.05110561 - time (sec): 354.24 - samples/sec: 7.98 - lr: 0.000029 - momentum: 0.000000\n",
      "2024-11-16 20:08:42,401 epoch 5 - iter 808/1017 - loss 0.04473267 - time (sec): 428.24 - samples/sec: 7.55 - lr: 0.000029 - momentum: 0.000000\n",
      "2024-11-16 20:09:49,828 epoch 5 - iter 909/1017 - loss 0.04068160 - time (sec): 495.66 - samples/sec: 7.34 - lr: 0.000028 - momentum: 0.000000\n",
      "2024-11-16 20:10:47,122 epoch 5 - iter 1010/1017 - loss 0.05058914 - time (sec): 552.96 - samples/sec: 7.31 - lr: 0.000028 - momentum: 0.000000\n",
      "2024-11-16 20:10:53,261 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-16 20:10:53,263 EPOCH 5 done: loss 0.0502 - lr: 0.000028\n",
      "2024-11-16 20:10:53,264 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:22<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 20:11:18,319 DEV : loss 0.05989818274974823 - f1-score (micro avg)  0.994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 32/32 [00:16<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 20:11:35,804 TEST : loss 0.05989818274974823 - f1-score (micro avg)  0.994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 20:11:35,990 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-16 20:12:34,658 epoch 6 - iter 101/1017 - loss 0.12197717 - time (sec): 58.66 - samples/sec: 6.89 - lr: 0.000027 - momentum: 0.000000\n",
      "2024-11-16 20:14:31,882 epoch 6 - iter 202/1017 - loss 0.09733798 - time (sec): 175.89 - samples/sec: 4.59 - lr: 0.000027 - momentum: 0.000000\n",
      "2024-11-16 20:15:18,425 epoch 6 - iter 303/1017 - loss 0.06563281 - time (sec): 222.43 - samples/sec: 5.45 - lr: 0.000026 - momentum: 0.000000\n",
      "2024-11-16 20:16:06,623 epoch 6 - iter 404/1017 - loss 0.07044911 - time (sec): 270.63 - samples/sec: 5.97 - lr: 0.000026 - momentum: 0.000000\n",
      "2024-11-16 20:16:56,629 epoch 6 - iter 505/1017 - loss 0.07142041 - time (sec): 320.64 - samples/sec: 6.30 - lr: 0.000025 - momentum: 0.000000\n",
      "2024-11-16 20:17:43,725 epoch 6 - iter 606/1017 - loss 0.06335902 - time (sec): 367.73 - samples/sec: 6.59 - lr: 0.000024 - momentum: 0.000000\n",
      "2024-11-16 20:18:36,280 epoch 6 - iter 707/1017 - loss 0.06292357 - time (sec): 420.29 - samples/sec: 6.73 - lr: 0.000024 - momentum: 0.000000\n",
      "2024-11-16 20:19:25,935 epoch 6 - iter 808/1017 - loss 0.05757493 - time (sec): 469.94 - samples/sec: 6.88 - lr: 0.000023 - momentum: 0.000000\n",
      "2024-11-16 20:20:16,850 epoch 6 - iter 909/1017 - loss 0.05151227 - time (sec): 520.86 - samples/sec: 6.98 - lr: 0.000023 - momentum: 0.000000\n",
      "2024-11-16 20:21:06,738 epoch 6 - iter 1010/1017 - loss 0.04642153 - time (sec): 570.74 - samples/sec: 7.08 - lr: 0.000022 - momentum: 0.000000\n",
      "2024-11-16 20:21:10,574 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-16 20:21:10,575 EPOCH 6 done: loss 0.0487 - lr: 0.000022\n",
      "2024-11-16 20:21:10,576 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:11<00:00,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 20:21:24,520 DEV : loss 0.4188661277294159 - f1-score (micro avg)  0.976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 32/32 [00:11<00:00,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 20:21:36,566 TEST : loss 0.4188661277294159 - f1-score (micro avg)  0.976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 20:21:36,730 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-16 20:22:22,282 epoch 7 - iter 101/1017 - loss 0.07931006 - time (sec): 45.55 - samples/sec: 8.87 - lr: 0.000022 - momentum: 0.000000\n",
      "2024-11-16 20:23:10,687 epoch 7 - iter 202/1017 - loss 0.06320913 - time (sec): 93.95 - samples/sec: 8.60 - lr: 0.000021 - momentum: 0.000000\n",
      "2024-11-16 20:23:59,014 epoch 7 - iter 303/1017 - loss 0.06611996 - time (sec): 142.28 - samples/sec: 8.52 - lr: 0.000021 - momentum: 0.000000\n",
      "2024-11-16 20:24:47,157 epoch 7 - iter 404/1017 - loss 0.06483480 - time (sec): 190.42 - samples/sec: 8.49 - lr: 0.000020 - momentum: 0.000000\n",
      "2024-11-16 20:25:35,860 epoch 7 - iter 505/1017 - loss 0.06054937 - time (sec): 239.13 - samples/sec: 8.45 - lr: 0.000019 - momentum: 0.000000\n",
      "2024-11-16 20:26:23,516 epoch 7 - iter 606/1017 - loss 0.05048654 - time (sec): 286.78 - samples/sec: 8.45 - lr: 0.000019 - momentum: 0.000000\n",
      "2024-11-16 20:27:11,880 epoch 7 - iter 707/1017 - loss 0.04592427 - time (sec): 335.15 - samples/sec: 8.44 - lr: 0.000018 - momentum: 0.000000\n",
      "2024-11-16 20:28:00,570 epoch 7 - iter 808/1017 - loss 0.04041619 - time (sec): 383.84 - samples/sec: 8.42 - lr: 0.000018 - momentum: 0.000000\n",
      "2024-11-16 20:29:29,270 epoch 7 - iter 909/1017 - loss 0.03939973 - time (sec): 472.54 - samples/sec: 7.69 - lr: 0.000017 - momentum: 0.000000\n",
      "2024-11-16 20:30:48,608 epoch 7 - iter 1010/1017 - loss 0.03566554 - time (sec): 551.88 - samples/sec: 7.32 - lr: 0.000017 - momentum: 0.000000\n",
      "2024-11-16 20:30:51,591 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-16 20:30:51,603 EPOCH 7 done: loss 0.0354 - lr: 0.000017\n",
      "2024-11-16 20:30:51,605 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:13<00:00,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 20:31:07,369 DEV : loss 0.14267194271087646 - f1-score (micro avg)  0.974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 32/32 [00:18<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 20:31:25,840 TEST : loss 0.14267194271087646 - f1-score (micro avg)  0.974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 20:31:26,035 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-16 20:32:19,288 epoch 8 - iter 101/1017 - loss 0.03482657 - time (sec): 53.25 - samples/sec: 7.59 - lr: 0.000016 - momentum: 0.000000\n",
      "2024-11-16 20:33:21,779 epoch 8 - iter 202/1017 - loss 0.04252398 - time (sec): 115.74 - samples/sec: 6.98 - lr: 0.000016 - momentum: 0.000000\n",
      "2024-11-16 20:34:27,909 epoch 8 - iter 303/1017 - loss 0.02839157 - time (sec): 181.87 - samples/sec: 6.66 - lr: 0.000015 - momentum: 0.000000\n",
      "2024-11-16 20:35:55,741 epoch 8 - iter 404/1017 - loss 0.03197704 - time (sec): 269.70 - samples/sec: 5.99 - lr: 0.000014 - momentum: 0.000000\n",
      "2024-11-16 20:37:00,296 epoch 8 - iter 505/1017 - loss 0.03760401 - time (sec): 334.26 - samples/sec: 6.04 - lr: 0.000014 - momentum: 0.000000\n",
      "2024-11-16 20:38:02,652 epoch 8 - iter 606/1017 - loss 0.04006775 - time (sec): 396.61 - samples/sec: 6.11 - lr: 0.000013 - momentum: 0.000000\n",
      "2024-11-16 20:39:14,075 epoch 8 - iter 707/1017 - loss 0.03966279 - time (sec): 468.04 - samples/sec: 6.04 - lr: 0.000013 - momentum: 0.000000\n",
      "2024-11-16 20:40:28,590 epoch 8 - iter 808/1017 - loss 0.04021371 - time (sec): 542.55 - samples/sec: 5.96 - lr: 0.000012 - momentum: 0.000000\n",
      "2024-11-16 20:41:41,035 epoch 8 - iter 909/1017 - loss 0.03577459 - time (sec): 615.00 - samples/sec: 5.91 - lr: 0.000012 - momentum: 0.000000\n",
      "2024-11-16 20:43:07,576 epoch 8 - iter 1010/1017 - loss 0.03712072 - time (sec): 701.54 - samples/sec: 5.76 - lr: 0.000011 - momentum: 0.000000\n",
      "2024-11-16 20:43:11,505 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-16 20:43:11,507 EPOCH 8 done: loss 0.0369 - lr: 0.000011\n",
      "2024-11-16 20:43:11,508 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:15<00:00,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 20:43:29,541 DEV : loss 0.013090575113892555 - f1-score (micro avg)  0.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 32/32 [00:13<00:00,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 20:43:43,322 TEST : loss 0.013090575113892555 - f1-score (micro avg)  0.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 20:43:43,520 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-16 20:44:37,023 epoch 9 - iter 101/1017 - loss 0.05217003 - time (sec): 53.50 - samples/sec: 7.55 - lr: 0.000011 - momentum: 0.000000\n",
      "2024-11-16 20:45:51,300 epoch 9 - iter 202/1017 - loss 0.03338340 - time (sec): 127.78 - samples/sec: 6.32 - lr: 0.000010 - momentum: 0.000000\n",
      "2024-11-16 20:46:30,149 epoch 9 - iter 303/1017 - loss 0.02688604 - time (sec): 166.63 - samples/sec: 7.27 - lr: 0.000009 - momentum: 0.000000\n",
      "2024-11-16 20:47:15,342 epoch 9 - iter 404/1017 - loss 0.02845709 - time (sec): 211.82 - samples/sec: 7.63 - lr: 0.000009 - momentum: 0.000000\n",
      "2024-11-16 20:48:09,159 epoch 9 - iter 505/1017 - loss 0.02889815 - time (sec): 265.64 - samples/sec: 7.60 - lr: 0.000008 - momentum: 0.000000\n",
      "2024-11-16 20:49:05,504 epoch 9 - iter 606/1017 - loss 0.02412729 - time (sec): 321.98 - samples/sec: 7.53 - lr: 0.000008 - momentum: 0.000000\n",
      "2024-11-16 20:49:52,057 epoch 9 - iter 707/1017 - loss 0.02239323 - time (sec): 368.54 - samples/sec: 7.67 - lr: 0.000007 - momentum: 0.000000\n",
      "2024-11-16 20:50:40,288 epoch 9 - iter 808/1017 - loss 0.02186902 - time (sec): 416.77 - samples/sec: 7.75 - lr: 0.000007 - momentum: 0.000000\n",
      "2024-11-16 20:51:36,656 epoch 9 - iter 909/1017 - loss 0.01945374 - time (sec): 473.13 - samples/sec: 7.68 - lr: 0.000006 - momentum: 0.000000\n",
      "2024-11-16 20:52:22,992 epoch 9 - iter 1010/1017 - loss 0.02145524 - time (sec): 519.47 - samples/sec: 7.78 - lr: 0.000006 - momentum: 0.000000\n",
      "2024-11-16 20:52:26,494 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-16 20:52:26,494 EPOCH 9 done: loss 0.0213 - lr: 0.000006\n",
      "2024-11-16 20:52:26,494 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:13<00:00,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 20:52:42,518 DEV : loss 0.009175065904855728 - f1-score (micro avg)  0.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 32/32 [00:12<00:00,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 20:52:55,446 TEST : loss 0.009175065904855728 - f1-score (micro avg)  0.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 20:52:55,598 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-16 20:53:59,663 epoch 10 - iter 101/1017 - loss 0.01194265 - time (sec): 64.06 - samples/sec: 6.31 - lr: 0.000005 - momentum: 0.000000\n",
      "2024-11-16 20:54:43,432 epoch 10 - iter 202/1017 - loss 0.01586806 - time (sec): 107.83 - samples/sec: 7.49 - lr: 0.000004 - momentum: 0.000000\n",
      "2024-11-16 20:55:33,748 epoch 10 - iter 303/1017 - loss 0.01074929 - time (sec): 158.15 - samples/sec: 7.66 - lr: 0.000004 - momentum: 0.000000\n",
      "2024-11-16 20:56:25,789 epoch 10 - iter 404/1017 - loss 0.02239072 - time (sec): 210.19 - samples/sec: 7.69 - lr: 0.000003 - momentum: 0.000000\n",
      "2024-11-16 20:57:20,297 epoch 10 - iter 505/1017 - loss 0.01797796 - time (sec): 264.70 - samples/sec: 7.63 - lr: 0.000003 - momentum: 0.000000\n",
      "2024-11-16 20:58:13,356 epoch 10 - iter 606/1017 - loss 0.01499590 - time (sec): 317.76 - samples/sec: 7.63 - lr: 0.000002 - momentum: 0.000000\n",
      "2024-11-16 20:59:07,135 epoch 10 - iter 707/1017 - loss 0.01706574 - time (sec): 371.54 - samples/sec: 7.61 - lr: 0.000002 - momentum: 0.000000\n",
      "2024-11-16 21:00:05,610 epoch 10 - iter 808/1017 - loss 0.01935944 - time (sec): 430.01 - samples/sec: 7.52 - lr: 0.000001 - momentum: 0.000000\n",
      "2024-11-16 21:00:58,105 epoch 10 - iter 909/1017 - loss 0.01949689 - time (sec): 482.51 - samples/sec: 7.54 - lr: 0.000001 - momentum: 0.000000\n",
      "2024-11-16 21:01:52,953 epoch 10 - iter 1010/1017 - loss 0.02099348 - time (sec): 537.35 - samples/sec: 7.52 - lr: 0.000000 - momentum: 0.000000\n",
      "2024-11-16 21:01:56,652 ----------------------------------------------------------------------------------------------------\n",
      "2024-11-16 21:01:56,652 EPOCH 10 done: loss 0.0209 - lr: 0.000000\n",
      "2024-11-16 21:01:56,652 Saving model at current epoch since 'save_model_each_k_epochs=1' was set\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:424] . unexpected pos 3408256 vs 3408150",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\serialization.py:619\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> 619\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\serialization.py:853\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    852\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[1;32m--> 853\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:593] . PytorchStreamWriter failed writing file data/0: file write failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m classifier \u001b[38;5;241m=\u001b[39m TextClassifier(document_embeddings, label_dictionary\u001b[38;5;241m=\u001b[39mlabel_dict, label_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m trainer \u001b[38;5;241m=\u001b[39m ModelTrainer(classifier, corpus)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfine_tune\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./models/gpt_data_v2/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmini_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmini_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmonitor_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_with_dev\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_model_each_k_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\flair\\trainers\\trainer.py:253\u001b[0m, in \u001b[0;36mModelTrainer.fine_tune\u001b[1;34m(self, base_path, warmup_fraction, learning_rate, decoder_learning_rate, mini_batch_size, eval_batch_size, mini_batch_chunk_size, max_epochs, optimizer, train_with_dev, train_with_test, reduce_transformer_vocab, main_evaluation_metric, monitor_test, monitor_train_sample, use_final_model_for_eval, gold_label_dictionary_for_eval, exclude_labels, sampler, shuffle, shuffle_first_epoch, embeddings_storage_mode, epoch, save_final_model, save_optimizer_state, save_model_each_k_epochs, create_file_logs, create_loss_file, write_weights, use_amp, plugins, attach_default_scheduler, **kwargs)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attach_default_scheduler:\n\u001b[0;32m    251\u001b[0m     plugins\u001b[38;5;241m.\u001b[39mappend(LinearSchedulerPlugin(warmup_fraction\u001b[38;5;241m=\u001b[39mwarmup_fraction))\n\u001b[1;32m--> 253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_custom(\n\u001b[0;32m    254\u001b[0m     base_path\u001b[38;5;241m=\u001b[39mbase_path,\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;66;03m# training parameters\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[0;32m    257\u001b[0m     decoder_learning_rate\u001b[38;5;241m=\u001b[39mdecoder_learning_rate,\n\u001b[0;32m    258\u001b[0m     mini_batch_size\u001b[38;5;241m=\u001b[39mmini_batch_size,\n\u001b[0;32m    259\u001b[0m     eval_batch_size\u001b[38;5;241m=\u001b[39meval_batch_size,\n\u001b[0;32m    260\u001b[0m     mini_batch_chunk_size\u001b[38;5;241m=\u001b[39mmini_batch_chunk_size,\n\u001b[0;32m    261\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mmax_epochs,\n\u001b[0;32m    262\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m    263\u001b[0m     train_with_dev\u001b[38;5;241m=\u001b[39mtrain_with_dev,\n\u001b[0;32m    264\u001b[0m     train_with_test\u001b[38;5;241m=\u001b[39mtrain_with_test,\n\u001b[0;32m    265\u001b[0m     reduce_transformer_vocab\u001b[38;5;241m=\u001b[39mreduce_transformer_vocab,\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;66;03m# evaluation and monitoring\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     main_evaluation_metric\u001b[38;5;241m=\u001b[39mmain_evaluation_metric,\n\u001b[0;32m    268\u001b[0m     monitor_test\u001b[38;5;241m=\u001b[39mmonitor_test,\n\u001b[0;32m    269\u001b[0m     monitor_train_sample\u001b[38;5;241m=\u001b[39mmonitor_train_sample,\n\u001b[0;32m    270\u001b[0m     use_final_model_for_eval\u001b[38;5;241m=\u001b[39muse_final_model_for_eval,\n\u001b[0;32m    271\u001b[0m     gold_label_dictionary_for_eval\u001b[38;5;241m=\u001b[39mgold_label_dictionary_for_eval,\n\u001b[0;32m    272\u001b[0m     exclude_labels\u001b[38;5;241m=\u001b[39mexclude_labels,\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;66;03m# sampling and shuffling\u001b[39;00m\n\u001b[0;32m    274\u001b[0m     sampler\u001b[38;5;241m=\u001b[39msampler,\n\u001b[0;32m    275\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[0;32m    276\u001b[0m     shuffle_first_epoch\u001b[38;5;241m=\u001b[39mshuffle_first_epoch,\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;66;03m# evaluation and monitoring\u001b[39;00m\n\u001b[0;32m    278\u001b[0m     embeddings_storage_mode\u001b[38;5;241m=\u001b[39membeddings_storage_mode,\n\u001b[0;32m    279\u001b[0m     epoch\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;66;03m# when and what to save\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     save_final_model\u001b[38;5;241m=\u001b[39msave_final_model,\n\u001b[0;32m    282\u001b[0m     save_optimizer_state\u001b[38;5;241m=\u001b[39msave_optimizer_state,\n\u001b[0;32m    283\u001b[0m     save_model_each_k_epochs\u001b[38;5;241m=\u001b[39msave_model_each_k_epochs,\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;66;03m# logging parameters\u001b[39;00m\n\u001b[0;32m    285\u001b[0m     create_file_logs\u001b[38;5;241m=\u001b[39mcreate_file_logs,\n\u001b[0;32m    286\u001b[0m     create_loss_file\u001b[38;5;241m=\u001b[39mcreate_loss_file,\n\u001b[0;32m    287\u001b[0m     write_weights\u001b[38;5;241m=\u001b[39mwrite_weights,\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;66;03m# amp\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     use_amp\u001b[38;5;241m=\u001b[39muse_amp,\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;66;03m# plugins\u001b[39;00m\n\u001b[0;32m    291\u001b[0m     plugins\u001b[38;5;241m=\u001b[39mplugins,\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    293\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\flair\\trainers\\trainer.py:674\u001b[0m, in \u001b[0;36mModelTrainer.train_custom\u001b[1;34m(self, base_path, learning_rate, decoder_learning_rate, mini_batch_size, eval_batch_size, mini_batch_chunk_size, max_epochs, optimizer, train_with_dev, train_with_test, max_grad_norm, reduce_transformer_vocab, main_evaluation_metric, monitor_test, monitor_train_sample, use_final_model_for_eval, gold_label_dictionary_for_eval, exclude_labels, sampler, shuffle, shuffle_first_epoch, embeddings_storage_mode, epoch, save_final_model, save_optimizer_state, save_model_each_k_epochs, create_file_logs, create_loss_file, write_weights, use_amp, plugins, **kwargs)\u001b[0m\n\u001b[0;32m    670\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPOCH \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m done: loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mlr_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    672\u001b[0m \u001b[38;5;66;03m# - CheckpointPlugin -> executes save_model_each_k_epochs\u001b[39;00m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;66;03m# - SchedulerPlugin -> log bad epochs\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mafter_training_epoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    678\u001b[0m \u001b[38;5;66;03m# Determine if this is the best model or if we need to anneal\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\flair\\trainers\\plugins\\base.py:110\u001b[0m, in \u001b[0;36mPluggable.dispatch\u001b[1;34m(self, event, *args, **kwargs)\u001b[0m\n\u001b[0;32m    107\u001b[0m         event, args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_queue\u001b[38;5;241m.\u001b[39mget()\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_handles[event]\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m--> 110\u001b[0m             hook(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# Reset the flag, since an exception event might be dispatched\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_processing_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\flair\\trainers\\plugins\\base.py:163\u001b[0m, in \u001b[0;36mHookHandle.__call__\u001b[1;34m(self, *args, **kw)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call the hook this `HookHandle` is associated with.\"\"\"\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    165\u001b[0m     sig \u001b[38;5;241m=\u001b[39m signature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\flair\\trainers\\plugins\\functional\\checkpoints.py:30\u001b[0m, in \u001b[0;36mCheckpointPlugin.after_training_epoch\u001b[1;34m(self, epoch, **kw)\u001b[0m\n\u001b[0;32m     25\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving model at current epoch since \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave_model_each_k_epochs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model_each_k_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwas set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     29\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_epoch_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(epoch) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_optimizer_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\flair\\nn\\model.py:126\u001b[0m, in \u001b[0;36mModel.save\u001b[1;34m(self, model_file, checkpoint)\u001b[0m\n\u001b[0;32m    123\u001b[0m     model_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_card\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_card\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# save model\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\serialization.py:618\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    615\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 618\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    619\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[0;32m    620\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\serialization.py:466\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 466\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    468\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:424] . unexpected pos 3408256 vs 3408150"
     ]
    }
   ],
   "source": [
    "document_embeddings = TransformerDocumentEmbeddings(model_name, fine_tune=True)\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict, label_type='label')\n",
    "\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "\n",
    "trainer.fine_tune(f'./models/gpt_data_v2/',\n",
    "                    learning_rate=learning_rate,\n",
    "                    mini_batch_size=mini_batch_size,\n",
    "                    max_epochs=max_epochs,\n",
    "                    monitor_test=True, train_with_dev=False, save_model_each_k_epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassifier.load(\"models/gpt_data/model_epoch_5.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:18<00:00, 13.66it/s]\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(corpus.test, gold_label_type=\"label\", mini_batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sentence for sentence in corpus.test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_tags = [sentence.tag for sentence in sentences]\n",
    "predicted_tags = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(sentences), 8):\n",
    "    sents = sentences[i:i+8]\n",
    "    model.predict(sents, mini_batch_size=8)\n",
    "    for sent in sents:\n",
    "        predicted_tags.append(sent.tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259\n",
      "NON_FOOD_GOODS FOOD_GOODS\n"
     ]
    }
   ],
   "source": [
    "for index in range(len(predicted_tags)):\n",
    "    if predicted_tags[index] != real_tags[index]:\n",
    "        print(index)\n",
    "        print(real_tags[index], predicted_tags[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence[15]: \"Оплата за Уход за одеждой и обувью по счету 11837472833255495630 от 21.06.2024г. Сумма 4110-00\" → NON_FOOD_GOODS (1.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.test[259]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "- F-score (micro) 0.998\n",
      "- F-score (macro) 0.9988\n",
      "- Accuracy 0.998\n",
      "\n",
      "By class:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "NON_FOOD_GOODS     1.0000    0.9896    0.9948        96\n",
      "    FOOD_GOODS     0.9890    1.0000    0.9945        90\n",
      "       SERVICE     1.0000    1.0000    1.0000        88\n",
      "  BANK_SERVICE     1.0000    1.0000    1.0000        49\n",
      "           TAX     1.0000    1.0000    1.0000        48\n",
      "          LOAN     1.0000    1.0000    1.0000        41\n",
      "       LEASING     1.0000    1.0000    1.0000        38\n",
      "   REALE_STATE     1.0000    1.0000    1.0000        27\n",
      "NOT_CLASSIFIED     1.0000    1.0000    1.0000        23\n",
      "\n",
      "      accuracy                         0.9980       500\n",
      "     macro avg     0.9988    0.9988    0.9988       500\n",
      "  weighted avg     0.9980    0.9980    0.9980       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result.detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (43358497.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    GPT != BERT -> вручную\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "GPT, BERT \n",
    "GPT != BERT -> вручную"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1) Квантизация 8\n",
    "2) оннх\n",
    "3) пайторч тредс"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
